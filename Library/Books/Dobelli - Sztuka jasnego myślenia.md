## Abstract

*The Art of Thinking Clearly is a collection of 99 short essays on common thinking errors that can lead to poor decision-making. Ranging from cognitive biases to fallacies and more, each chapter offers insights and guidance on how to avoid these pitfalls and think more clearly.*
# Notatki: 2023-06-08 21:05:55 - The Art of Thinking Clearly

Rolf Dobelli

14

Survivorship bias means this: People systematically overestimate their chances of success.

20

When it comes to pattern recognition, we are oversensitive.

21

Social proof, sometimes roughly termed the “herd instinct,” dictates that individuals feel they are behaving correctly when they act the same as other people.

23

W. Somerset Maugham’s wise words: “If fifty million people say something foolish, it is still foolish.

30

“Facts do not cease to exist because they are ignored,” said writer Aldous Huxley.

30

The confirmation bias is the mother of all misconceptions. It is the tendency to interpret new information so that it becomes compatible with our existing theories, beliefs, and convictions.

34

Self-help and get-rich-quick books are further examples of blinkered storytelling. Their shrewd authors collect piles of proof to pump up the most banal of theories, such as “meditation is the key to happiness.”

35

Internet is particularly fertile ground for the confirmation bias. To stay informed, we browse news sites and blogs, forgetting that our favored pages mirror our existing values,

42

The availability bias says this: We create a picture of the world using the examples that most easily come to mind.

44

We prefer wrong information to no information.

50

From our own life stories to global events, we shape everything into meaningful stories.

50

Whenever you hear a story, ask yourself: Who is the sender, what are his intentions, and what did he hide under the rug?

51

The hindsight bias is one of the most prevailing fallacies of all. We can aptly describe it as the “I told you so” phenomenon: In retrospect, everything seems clear and inevitable.

52

So why is the hindsight bias so perilous? Well, it makes us believe we are better predictors than we actually are, causing us to be arrogant about our knowledge and consequently to take too much risk.

53

Overcoming the hindsight bias is not easy. Studies have shown that people who are aware of it fall for it just as much as everyone else. So, I’m very sorry, but you’ve just wasted your time reading this chapter.

53

If you’re still with me, I have one final tip, this time from personal rather than professional experience: Keep a journal. Write down your predictions—for political changes, your career, your weight, the stock market, and so on. Then, from time to time, compare your notes with actual developments. You will be amazed at what a poor forecaster you are.

57

After receiving the Nobel Prize in Physics in 1918, Max Planck went on tour across Germany. Wherever he was invited, he delivered the same lecture on new quantum mechanics. Over time, his chauffeur grew to know it by heart: “It has to be boring giving the same speech each time, Professor Planck. How about I do it for you in Munich? You can sit in the front row and wear my chauffeur’s cap. That’d give us both a bit of variety.” Planck liked the idea, so that evening the driver held a long lecture on quantum mechanics in front of a distinguished audience. Later, a physics professor stood up with a question. The driver recoiled: “Never would I have thought that someone from such an advanced city as Munich would ask such a simple question! My chauffeur will answer it.”

59

One of Munger’s best pieces of advice is: “You have to stick within what I call your circle of competence. You have to know what you understand and what you don’t understand. It’s not terribly important how big the circle is. But it is terribly important that you know where the perimeter is.”

59

True experts recognize the limits of what they know and what they do not know. If they find themselves outside their circle of competence, they keep quiet or simply say, “I don’t know.”

61

Crossing the street in Los Angeles is a tricky business, but luckily, at the press of a button, we can stop traffic. Or can we? The button’s real purpose is to make us believe we have an influence on the traffic lights, and thus we’re better able to endure the wait for the signal to change with more patience. The same goes for “door-open” and “door-close” buttons in elevators: Many are not even connected to the electrical panel. Such tricks are also designed in open-plan offices: For some people it will always be too hot, for others, too cold. Clever technicians create the illusion of control by installing fake temperature dials. This reduces energy bills—and complaints. Such ploys are called “placebo buttons” and they are being pushed in all sorts of realms.

64

Making CEO pay public didn’t dampen the astronomical salaries; to the contrary, it pushed them upward. Nobody wants to be the loser CEO in his industry.

70

the outcome bias: We tend to evaluate decisions based on the result rather than on the decision process. This fallacy is also known as the “historian error.

73

surfeit of choices destroys quality of life. The technical term for this is the paradox of choice.  
In his book of the same title, psychologist Barry Schwartz describes why this is so. First, a large selection leads to inner paralysis.

73

Second, a broader selection leads to poorer decisions.

73

a supermarket set up a stand where customers could sample twenty-four varieties of jelly. They could try as many as they liked and then buy them at a discount. The next day, the owners carried out the same experiment with only six flavors. The result? They sold ten times more jelly on day two. Why? With such a wide range, customers could not come to a decision, so they bought nothing. The experiment was repeated several times with different products. The results were always the same.

74

Finally, large selection leads to discontent. How can you be sure you are making the right choice when two hundred options surround and confound you? The answer is: You cannot. The more choice you have, the more unsure and therefore dissatisfied you are afterward.

81

At 7:15 p.m. on March 1, 1950, the fifteen members of the church choir in Beatrice, Nebraska, were scheduled to meet for rehearsal. For various reasons, they were all running late. The minister’s family was delayed because his wife still had to iron their daughter’s dress. One couple was held back when their car wouldn’t start. The pianist wanted to be there thirty minutes early, but he fell into a deep sleep after dinner. And so on. At 7:25 p.m., the church exploded. The blast was heard all around the village. It blew out the walls and sent the roof crashing to the ground. Miraculously, nobody was killed. The fire chief traced the explosion back to a gas leak, even though members of the choir were convinced they had received a sign from God. Hand of God or coincidence?

85

Psychology professor Irving Janis has studied many fiascoes. He concluded that they share the following pattern: Members of a close-knit group cultivate team spirit by (unconsciously) building illusions. One of these fantasies is a belief in invincibility: “If both our leader [in this case, Kennedy] and the group are confident that the plan will work, then luck will be on our side.” Next comes the illusion of unanimity: If the others are of the same opinion, any dissenting view must be wrong. No one wants to be the naysayer that destroys team unity. Finally, each person is happy to be part of the group. Expressing reservations could mean exclusion from it. In our evolutionary past, such banishment guaranteed death; hence our strong urge to remain in the group’s favor.  
Groupthink is no stranger in the business world.

86

If you ever find yourself in a tight, unanimous group, you must speak your mind, even if your team does not like it. Question tacit assumptions, even if you risk expulsion from the warm nest. And, if you lead a group, appoint someone as devil’s advocate. She will not be the most popular member of the team, but she might be the most important.

88

we respond to the expected magnitude of an event (the size of the jackpot or the amount of electricity), but not to its likelihood. In other words: We lack an intuitive grasp of probability.  
The proper term for this is neglect of probability, and it leads to errors in decision making.

115

when companies buy other companies—the infamous mergers and acquisitions—the winner’s curse is present more often than not. Astoundingly, more than half of all acquisitions destroy value, according to a McKinsey study.

116

In conclusion: Accept this piece of wisdom about auctions from Warren Buffett: “Don’t go.” If you happen to work in an industry where they are inevitable, set a maximum price and deduct 20 percent from this to offset the winner’s curse. Write this number on a piece of paper and don’t go a cent over it.

116

So how much would you pay for $100? Imagine that you and an opponent are invited to take part in such an auction. The rules: Whoever makes the highest offer gets the $100 bill, and—most important—when this happens, both bidders have to pay their final offer. How high will you go? From your perspective, it makes sense to pay $20, $30, or $40. Your opponent does the same. Even $99 seems like a reasonable offer for a $100 bill. Now, your competitor offers $100. If this remains the highest bid, he will come away breaking even (paying $100 for $100), whereas you will simply have to cough up $99. So you continue to bid. At $110, you have a guaranteed loss of $10, but your opponent would have to shell out $109 (his last bid). So he will continue playing. When do you stop? When will your competitor give up? Try it out with friends.

119

our lives depended on and revolved around others, which explains why we are so obsessed with our fellow humans today. The result of this infatuation is that we spend about 90 percent of our time thinking about other people and dedicate just 10 percent to assessing other factors and contexts.

129

Philip Tetlock. Over a period of ten years, he evaluated 28,361 predictions from 284 self-appointed professionals. The result: In terms of accuracy, the experts fared only marginally better than a random forecast generator. Ironically, the media darlings were among the poorest performers; and of those, the worst were the prophets of doom and disintegration. Examples of their far-fetched forecasts included the collapse of Canada, Nigeria, China, India, Indonesia, South Africa, Belgium, and the EU. None of these countries has imploded.

129

“There are two kinds of forecasters: those who don’t know, and those who don’t know they don’t know,” wrote Harvard economist John Kenneth Galbraith.

132

The conjunction fallacy is at play when such a subset seems larger than the entire set—which by definition cannot be the case. Amos Tversky and Nobel laureate Daniel Kahneman have studied this extensively.  
We are easy prey for the conjunction fallacy because we have an innate attraction to “harmonious” or “plausible” stories.

Eksperyment "Linda"

134

Kahneman believes that two types of thinking exist: The first kind is intuitive, automatic, and direct. The second is conscious, rational, slow, laborious, and logical. Unfortunately, intuitive thinking draws conclusions long before the conscious mind does.

134

In conclusion: Forget about left brains and right brains: The difference between intuitive and conscious thinking is much more significant. With important decisions, remember that, at the intuitive level, we have a soft spot for plausible stories. Therefore, be on the lookout for convenient details and happy endings. Remember: If an additional condition has to be met, no matter how plausible it sounds, it will become less, not more, likely.

135

Bookmark

136

“Glossing” is a popular type of framing. Under its rules, a tumbling share price becomes a “correction.” An overpaid acquisition price is branded “goodwill.” In every management course, a problem magically transforms into an “opportunity” or a “challenge.” A person who is fired is “reassessing his career.” A fallen soldier—regardless of how much bad luck or stupidity led to his death—turns into a “war hero.” Genocide translates to “ethnic cleansing.” A successful emergency landing, for example on the Hudson River, is celebrated as a “triumph of aviation.”

Glossing - wyjaśnianie, połysk, korzystne światło

136

Researchers presented a group of people with two kinds of meat, “99 percent fat free” and “1 percent fat,” and asked them to choose which was healthier. Can you guess which they picked? Bingo: Respondents ranked the first type of meat as healthier, even though both were identical. Next came the choice between “98 percent fat free” and “1 percent fat.” Again, most respondents chose the first option—despite its higher fat content.

138

This is the action bias: Look active, even if it achieves nothing.  
This study comes from the Israeli researcher Michael Bar-Eli, who evaluated hundreds of penalty shoot-outs.

Bramkarze przy karnych

139

Lightning-fast reactions were essential to survival; deliberation could be fatal. When our ancestors saw a silhouette appear at the edge of the forest—something that looked a lot like a saber-toothed tiger—they did not take a pew to muse over what it might be. They hit the road—and fast. We are the descendants of these quick responders. Back then, it was better to run away once too often. However, our world today is different; it rewards reflection, even though our instincts may suggest otherwise.

140

“All of humanity’s problems stem from man’s inability to sit quietly in a room alone,” wrote Blaise Pascal. At home, in his study.

141

This feeling is called the omission bias. It crops up where both action and inaction lead to cruel consequences. In such cases, we tend to prefer inaction; its results seem more anodyne.

142

Active euthanasia, even if it is the explicit wish of the dying, is punishable by law, whereas deliberate refusal of lifesaving measures is legal (for example, following so-called DNR orders—do not resuscitate).

144

We attribute success to ourselves and failures to external factors. This is the self-serving bias.

146

In married couples, the same thing happens: It’s been shown that both men and women overestimate their contribution to the health of the marriage. Each assumes their input is more than 50 percent.

146

how can we dodge the self-serving bias? Do you have friends who tell you the truth—no holds barred? If so, consider yourself lucky. If not, do you have at least one enemy? Good. Invite him or her over for coffee and ask for an honest opinion about your strengths and weaknesses. You will be forever grateful you did.

146

I decided to ask them separately how often they take out the trash. One said he did it every second time. Another: every third time. Roommate number 3, cursing because his garbage bag had split, reckoned he did it pretty much every time, say 90 percent. Although their answers should have added up to 100 percent, these boys achieved an impressive 320 percent! The five systematically overestimated their roles—

151

Whenever we complain about bad luck, we must be wary of the so-called self-selection bias.

160

Leon Festinger and James M. Carlsmith of Stanford University once asked their students to carry out an hour of excruciatingly boring tasks. They then divided the subjects into two groups. Each student in group A received a dollar (it was 1959) and instructions to wax lyrical about the work to another student waiting outside—in other words, to lie. The same was asked of the students in group B, with one difference: They were given $20 for the task. Later, the students had to divulge how they really found the monotonous work. Interestingly, those who received only a dollar rated it as significantly more enjoyable and interesting. Why? One measly dollar was not enough for them to lie outright; instead they convinced themselves that the work was not that bad. Just as Aesop’s fox reinterpreted the situation, so did they. The students who received more didn’t have to justify anything. They had lied and netted $20 for it—a fair deal. They experienced no cognitive dissonance.

163

Hyperbolic discounting, the fact that immediacy magnetizes us, is a remnant of our animal past. Animals will never turn down an instant reward in order to attain more in the future. You can train rats as much as you like; they’re never going to give up a piece of cheese today to get two pieces tomorrow.

182

the former CEO of General Electric Jack Welch. He once said in an interview: “You would not believe how difficult it is to be simple and clear. People are afraid that they may be seen as a simpleton. In reality, just the opposite is true.

182

Verbal expression is the mirror of the mind. Clear thoughts become clear statements, whereas ambiguous ideas transform into vacant ramblings. The trouble is that, in many cases, we lack very lucid thoughts.

184

the Will Rogers phenomenon, named after an American comedian from Oklahoma. He is said to have joked that Oklahomans who pack up and move to California raise both states’ average IQ.

188

Forget trying to amass all the data. Do your best to get by with the bare facts. It will help you make better decisions. Superfluous knowledge is worthless, whether you know it or not. The historian Daniel J. Boorstin put it right: “The greatest obstacle to discovery is not ignorance—it is the illusion of knowledge.” And next time you are confronted by a rival, consider killing him—not with kindness but with reams of data and analysis.

189

effort justification. When you put a lot of energy into a task, you tend to overvalue the result.

190

A mild form of effort justification is the so-called IKEA effect. Furniture that we assemble ourselves seems more valuable than any expensive designer piece. The same goes for hand-knitted socks.

191

Now that you know about effort justification, you can rate your projects more objectively. Try it out: Whenever you have invested a lot of time and effort into something, stand back and examine the result—only the result.

196

In 1965, the American psychologist Robert Rosenthal conducted a noteworthy experiment in various schools. Teachers were told of a (fake) new test that could identify students who were on the verge of an intellectual spurt—so-called bloomers. Twenty percent of students were randomly selected and classified as such. Teachers remained under the impression that these were indeed high-potential students.

198

Thousands of people have taken this Cognitive Reflection Test (CRT), which professor Shane Frederick developed.

198

First question: In a department store, a Ping-Pong paddle and a plastic ball cost $1.10. If the paddle costs $1 more, how much is the ball? Second question: In a textile factory, five machines take exactly five minutes to make five shirts. How many minutes will it take one hundred machines to produce one hundred shirts? And, the third question: A pond has water lilies growing in it. The flowers multiply quickly, each day doubling the area they take up. If it takes forty-eight days for the pond to be completely covered with water lilies, how many days will it take for it to be half covered?

202

The Forer effect explains why the pseudosciences work so well—astrology, astrotherapy, the study of handwriting, biorhythm analysis, palmistry, tarot card readings, and séances with the dead.  
What’s behind the Forer effect? First, the majority of statements in Forer’s passage are so general that they relate to everyone: “Sometimes you seriously doubt your actions.” Who doesn’t? Second, we tend to accept flattering statements that don’t apply to us: “You are proud of your independent thinking.” Obviously! Who sees himself or herself as a mindless follower? Third, the so-called feature-positive effect plays a part: The text contains no negative statements; it states only what we are, even though the absence of characteristics is an equally important part of a person’s makeup. Fourth, the father of all the fallacies, the confirmation bias: We accept whatever corresponds to our self-image and unconsciously filter everything else out. What remains is a coherent portrait.

217

How will the world look in fifty years? In his latest book, Antifragile, Nassim Taleb gives us a clue: Assume that most of the technology that has existed for the past fifty years will serve us for another half century. And assume that recent technology will be passé in a few years’ time.

229

If you have an opinion, don’t hesitate airing it first.

230

In conclusion: First and last impressions dominate, meaning the content sandwiched between has only a weak influence. Try to avoid evaluations based on first impressions. They will deceive you, guaranteed, in one way or another. Try to assess all aspects impartially. It’s not easy, but there are ways around it.

240

We frequently overestimate unanimity with others, believing that everyone else thinks and feels exactly like we do. This fallacy is called the false-consensus effect.

241

The false-consensus effect is fascinating for yet another reason. If people do not share our opinions, we categorize them as “abnormal.”

242

With the false-consensus effect, no outside influences are involved. Despite this, it still has a social function, which is why evolution didn’t eliminate it. Our brain is not built to recognize the truth; instead, its goal is to leave behind as many offspring as possible. Whoever seemed courageous and convincing (thanks to the false-consensus effect) created a positive impression, attracted a disproportionate amount of resources, and thus increased their chances of passing on their genes to future generations. Doubters were less sexy.

243

In 1973, U.S. political scientist Gregory Markus asked three thousand people to share their opinions on controversial political issues, such as the legalization of drugs. Their responses ranged from “fully agree” to “completely disagree.” Ten years later, he interviewed them again on the same topics, and also asked what they had replied ten years previously. The result: What they recalled disclosing in 1973 was almost identical to their present-day views—and a far cry from their original responses.

245

Ulric Neisser, one of the pioneers in the field of cognitive science, investigated them: In 1986, the day after the explosion of the Challenger space shuttle, he asked students to write essays detailing their reactions. Three years later, he interviewed them again. Less than 7 percent of the new data correlated with the initial submissions. In fact, 50 percent of the recollections were incorrect in two-thirds of the points, and 25 percent failed to match even a single detail. Neisser took one of these conflicting papers and presented it to its owner. Her answer: “I know it’s my handwriting, but I couldn’t have written this.

245

It is safe to assume that half of what you remember is wrong.

247

First, groups often form based on minor, even trivial, criteria.

247

Second, you perceive people outside your own group to be more similar than they actually are. This is called the “out-group homogeneity bias.

247

British psychologist Henri Tajfel split strangers into groups, tossing a coin to choose who went to which group. He told the members of one group it was because they all liked a particular type of art. The results were impressive: Although (a) they were strangers, (b) they were allocated a group at random, and (c) they were far from art connoisseurs, the group members found each other more agreeable than members of other groups.

248

Third, since groups often form on the basis of common values, group members receive a disproportionate amount of support for their own views. This distortion is dangerous, especially in business: It leads to the infamous organizational blindness.

249

Ellsberg Paradox offers empirical proof that we favor known probabilities (box A) over unknown ones (box B).

265

Willpower is like a battery, at least in the short term. If it is depleted, future challenges will falter.  
This is a fundamental insight. Self-control is not available around the clock. It needs time to refuel. The good news: To achieve this, all you need to do is refill your blood sugar and kick back and relax.

266

Dan Ariely found that dates stipulated by external authorities—for example, a teacher or the IRS—work best. Self-imposed deadlines will work only if the task is broken down step-by-step, with each part assigned its own due date.

271

You would think that people’s generosity would grow if they knew the extent of the disaster. But we do not function like that. Statistics don’t stir us; people do.  
The media has long known that factual reports and bar charts do not entice readers. Hence the guideline: Give the story a face.

277

Most vulnerable to strategic misrepresentation are mega-projects, where (a) accountability is diffuse (for example, if the administration that commissioned the project is no longer in power), (b) many businesses are involved, leading to mutual finger-pointing, or (c) the end date is a few years down the road.

277

strategic misrepresentation: the more at stake, the more exaggerated your assertions become.

280

if you think too much, you cut off your mind from the wisdom of your feelings. This may sound a little esoteric—and a bit surprising coming from someone like me who strives to rid my thinking of irrationality—but it is not. Emotions form in the brain, just as crystal-clear, rational thoughts do. They are merely a different form of information processing—more primordial, but not necessarily an inferior variant. In fact, sometimes they provide the wiser counsel.

281

When do you listen to your head and when do you heed your gut? A rule of thumb might be: If it is something to do with practiced activities, such as motor skills (think of the centipede, Van de Velde, or mastering a musical instrument) or questions you’ve answered a thousand times (think of Warren Buffett’s “circle of competence”), it’s better not to reflect to the last detail. It undermines your intuitive ability to solve problems.

281

With complex matters, though, such as investment decisions, sober reflection is indispensable. Evolution has not equipped us for such considerations, so logic trumps intuition.

281

For such purposes, we have heuristics, mental shortcuts that are clearly superior to rational thought.

283

Sydney Opera House was planned in 1957: Completion was due in 1963 at a cost of $7 million. It finally opened its doors in 1973 after $102 million had been pumped in—fourteen times the original estimate!

283

The planning fallacy is particularly evident when people work together—in business, science, and politics. Groups overestimate duration and benefits and systematically underestimate costs and risks.

284

psychologist Gary Klein recommends delivering this short speech to the assembled team: “Imagine it is a year from today. We have followed the plan to the letter. The result is a disaster. Take five or ten minutes to write about this disaster.”

pre mortem

285

“If your only tool is a hammer, all your problems will be nails,” said Mark Twain—a quote that sums up the déformation professionnelle perfectly.

288

once we’ve completed a task and checked it off our mental list, it is erased from memory.  
The researcher has lent her name to this: Scientists now speak of the Zeigarnik effect.

296

In conclusion: We have problems perceiving nonevents. We are blind to what does not exist.

296

If we thought more frequently about absence, we might well be happier. But it is tough mental work.

296

In academia, we constantly encounter the feature-positive effect. The confirmation of hypotheses leads to publications, and in exceptional cases these are rewarded with Nobel Prizes. On the other hand, the falsification of a hypothesis is a lot harder to get published, and as far as I know, there has never been a Nobel Prize awarded for this. However, such falsification is as scientifically valuable as confirmation.

297

cherry picking: selecting and showcasing the most attractive features and hiding the rest.

299

over time, the original goals have faded. These have been replaced, quietly and secretly, with self-set goals that are always attainable.

301

How do you go about investigating the cause? First, you know that there will never be one sole factor. Take a sheet of paper and sketch out all the potential reasons. Do the same for the reasons behind these reasons. After a while, you will have a network of possible influencing factors. Second, highlight those you can change and delete those you cannot (such as “human nature”). Third, conduct empirical tests by varying the highlighted factors in different markets. This costs time and money, but it’s the only way to escape the swamp of superficial assumptions.

302

The notion of free will is up for debate. Our actions are brought about by the interaction of thousands of factors—from genetic predisposition to upbringing, from education to the concentration of hormones between individual brain cells. Still we hold firmly to the old image of self-governance. This is not only wrong but also morally questionable. As long as we believe in singular reasons, we will always be able to trace triumphs or disasters back to individuals and stamp them “responsible.”

306

The first weeks were hard. Very hard. I was constantly afraid of missing something. But after a while, I had a new outlook. The result after three years: clearer thoughts, more valuable insights, better decisions, and much more time. And the best thing? I haven’t missed anything important. My social network—not Facebook, the one that exists in the real world consisting of flesh-and-blood friends and acquaintances—works as a news filter and keeps me in the loop.

odstawienie newsów

306

We are incredibly well informed, yet we know incredibly little. Why? Because two centuries ago, we invented a toxic form of knowledge called “news.” News is to the mind what sugar is to the body: appetizing, easy to digest—and highly destructive in the long run.

307

Everything subtle, complex, abstract, and profound must be systematically filtered out, even though such stories are much more relevant to our lives and to our understanding of the world. As a result of news consumption, we walk around with a distorted mental map of the risks and threats we actually face.

307

First, our brains react disproportionately to different types of information.

307

Second, news is irrelevant. In the past twelve months, you have probably consumed about ten thousand news snippets—perhaps as many as thirty per day. Be very honest: Name one of them, just one that helped you make a better decision—for your life, your career, or your business—compared with not having this piece of news. No one I have asked has been able to name more than two useful news stories—out of ten thousand. A miserable result.

308

Third, news is a waste of time. An average human being squanders half a day each week on reading about current affairs. In global terms, this is an immense loss of productivity.

309

The pope asked Michelangelo: “Tell me the secret of your genius. How have you created the statue of David, the masterpiece of all masterpieces?” Michelangelo’s answer: “It’s simple. I removed everything that is not David.

309

We don’t know for sure what makes us successful. We can’t pinpoint exactly what makes us happy. But we know with certainty what destroys success or happiness. This realization, as simple as it is, is fundamental: Negative knowledge (what not to do) is much more potent than positive knowledge (what to do).

309

The Greeks, Romans, and medieval thinkers had a term for this approach: via negativa. Literally, the negative path, the path of renunciation, of exclusion, of reduction. Theologians were the first to tread the via negativa: We cannot say what God is; we can only say what God is not. Applied to the present day: We cannot say what brings us success. We can pin down only what blocks or obliterates success. Eliminate the downside, the thinking errors, and the upside will take care of itself. This is all we need to know.

310

The hot theory is as old as the hills. Here is Plato’s analogy: A rider steers wildly galloping horses; the rider signifies reason and the galloping horses embody emotions. Reason tames feelings. If this fails, irrationality runs free. Another example: Feelings are like bubbling lava. Usually, reason can keep a lid on them, but every now and then the lava of irrationality erupts. Hence hot irrationality.

310

Two theories of irrationality exist: a hot and a cold.

310

According to Austrian psychoanalyst Sigmund Freud’s theory, the rationalist “ego” and the moralistic “superego” control the impulsive “id.” But that theory holds less water in the real world. Forget about obligation and discipline. To believe that we can completely control our emotions through thinking is illusory—as illusory as trying to make your hair grow by willing it to.

311

a cold theory of irrationality that states: Thinking is in itself not pure, but prone to error. This affects everyone. Even highly intelligent people fall into the same cognitive traps. Likewise, errors are not randomly distributed. We systematically err in the same direction. That makes our mistakes predictable, and thus fixable to a degree—but only to a degree, never completely.

311

Thinking is a biological phenomenon. Evolution has shaped it just as it has the forms of animals or the colors of flowers.

311

Suppose we could go back fifty thousand years, grab hold of an ancestor, and bring him back with us into the present. We send him to the hairdresser and put him in a Hugo Boss suit. Would he stand out on the street? No. Of course, he would have to learn English, how to drive, and how to operate a cell phone, but we had to learn those things, too. Biology has dispelled all doubt: Physically, and that includes cognitively, we are hunter-gatherers in Hugo Boss (or H&M, as the case may be).

:)

311

the cold theory of irrationality is still young. After the Second World War, many searched for explanations about the irrationality of the Nazis. Emotional outbursts were rare in Hitler’s leadership ranks. Even his fiery speeches were nothing more than masterful performances. It was not molten eruptions but stone-cold calculation that resulted in the Nazi madness. The same can be said of Stalin or of the Khmer Rouge.

311

Physically, and that includes cognitively, we are hunter-gatherers in Hugo Boss

312

The result is overwhelming material prosperity, but also lifestyle diseases (such as type 2 diabetes, lung cancer, and depression) and errors in thinking.

312

In the past ten thousand years, we have created a world that we no longer understand.

312

Everything is more sophisticated, but also more complex and interdependent.

312

What has changed markedly since ancient times is the environment in which we live. Back then, things were simple and stable. We lived in small groups of about fifty people. There was no significant technological or social progress. Only in the last ten thousand years did the world begin to transform dramatically, with the development of crops, livestock, villages, cities, global trade, and financial markets. Since industrialization, little is left of the environment for which our brain is optimized.

312

If the complexity continues to rise—and it will, that much is certain—these errors will only increase and intensify.

313

Why is that? Evolution does not “optimize” us completely.

313

A second, parallel explanation of why our mistakes are so persistent took shape in the late 1990s: Our brains are designed to reproduce rather than search for the truth. In other words, we use our thoughts primarily to persuade. Whoever convinces others secures power and thus access to resources. Such assets represent a major advantage for mating and for rearing offspring.

314

we often decide intuitively and justify our choices later. Many decisions (career, life partner, investments) take place subconsciously. A fraction of a second later, we construct a reason so that we feel we made a conscious choice. Alas, we do not behave like scientists who are purely interested in objective facts. Instead, we think like lawyers, crafting the best possible justification for a predetermined conclusion.

314

forget about the “left and right brain” that semi-intelligent self-help books describe. Much more important is the difference between intuitive and rational thinking. Both have legitimate applications. The intuitive mind is swift, spontaneous, and energy-saving. Rational thinking is slow, demanding, and energy-guzzling (in the form of blood sugar). Nobody has described this better than the great Daniel Kahneman in Thinking, Fast and Slow.

314

a third explanation exists: Intuitive decisions, even if they lack logic, are better under certain circumstances. So-called heuristic research deals with this topic. For many decisions, we lack the necessary information, so we are forced to use mental shortcuts and rules of thumb (heuristics)

315

In situations where the consequences are small (i.e., regular or Diet Pepsi, sparkling or flat water?), I forget about rational optimization and let my intuition take over.

315

To make things simple, I have set myself the following rules: In situations where the possible consequences are large (i.e., important personal or business decisions), I try to be as reasonable and rational as possible when choosing. I take out my list of errors and check them off one by one, just like a pilot does. I’ve created a handy checklist decision tree, and I use it to examine important decisions